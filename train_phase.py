# -*- coding: utf-8 -*-
"""IR_project_indo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15U5Vz5JSaize7LJLBsQVSA3M7kZI26I6
"""

pip install Sastrawi

!unzip "/content/drive/MyDrive/Colab Notebooks/IR/archive.zip"

import nltk
nltk.download('punkt')
nltk.download('stopwords')
import Sastrawi
import numpy as np
import pandas as pd
import re
from nltk.tokenize import word_tokenize 
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.tokenize.treebank import TreebankWordDetokenizer as Detok
from scipy import spatial
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from sklearn.metrics import confusion_matrix 
import pickle

def clean_str(string):
  """
  Tokenization/string cleaning for all datasets except for SST.
  Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py
  """
  string = re.sub(r"[^A-Za-z0-9()\,!?\'\`]", " ", string)
  string = re.sub(r"[0-9!@#$&()\\-`.+,/\"]", " ", string)
  string = re.sub(r",", " , ", string)
  string = re.sub(r"!", " ! ", string)
  string = re.sub(r"\(", " \( ", string)
  string = re.sub(r"\)", " \) ", string)
  string = re.sub(r"\?", " \? ", string)
  string = re.sub(r"\s{2,}", " ", string)

  return string.strip().lower()

def importData(path, columns, label):
  columns.append(label);
  df = pd.read_csv(path)
  df= df.drop(df.columns.difference(columns), axis=1).dropna()
  np.random.seed(10)

  #only using 1% of the data so it doesn't take too long to train
  remove_n = int(df.shape[0]*0.99)
  drop_indices = np.random.choice(df.index, remove_n, replace=False)
  df_subset = df.loc[drop_indices, :]
  df = df.drop(drop_indices)

  return df

def detokenize(string):
  detokenizer = Detok()
  temp = detokenizer.detokenize(string)
  return temp

def preprocessing(string, stemmer):
  #casefolding and string cleaning
  string = clean_str(string)
  #tokenizing1
  string = nltk.tokenize.word_tokenize(string)
  # print("selesai tokenizing", string)
  #no need for stopwords removal because the TF-IDF will remove it

  #stemming
  string = [stemmer.stem(sent) for sent in string]
  # print("selesai stemming",string)
  #detokenizing preprocessed dataset to use as an input for TfidfVectorizer
  string = detokenize(string)

  return string

def define_rating(rating):
  if rating < 3:
    return 'negatif'
  elif rating > 3:
    return 'positif'
  else:
    return 'netral'

def label_preprocess(data):
  data = [define_rating(rating) for rating in data];
  
  return data

def load_train_test(x, y):
    xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.33, random_state=42)

    return xtrain, xtest, ytrain, ytest

if __name__ == "__main__":
  columns = ['category','reviewContent'] #change into the column name where exists the data that you want to process
  label_column = 'rating' #COLUMN NAME FOR LABEL/TARGET
  

  #path to file 20191002-reviews.csv
  #dataset from https://www.kaggle.com/grikomsn/lazada-indonesian-reviews
  path = '/content/drive/MyDrive/Colab Notebooks/IR/20191002-reviews.csv' #csv path


  factory = StemmerFactory()
  stemmer = factory.create_stemmer()
  data = importData(path, columns, label_column)
  reviews = data[columns[0]] + " " + data[columns[1]]
  reviews = [preprocessing(string, stemmer) for string in reviews]
  labels = label_preprocess(data[label_column])

  df = pd.DataFrame({'reviews': reviews,
                     'labels': labels})

  df.to_csv(r'dataset.csv', index = False)

  vectorizer = TfidfVectorizer(min_df = 5, max_df = 0.8)
  X = vectorizer.fit_transform(np.array(df['reviews']))
  xtrain, xtest, ytrain, ytest = train_test_split(X, df['labels'], test_size=0.33, random_state=42)
  

  model = svm.SVC(kernel='rbf')
  model.fit(xtrain, ytrain)
  
  prediction = model.predict(xtest)

  #accuracy and confusion matrix
  accuracy = model.score(xtest, ytest) 
  cm = confusion_matrix(ytest, prediction)
  print("accuracy:", accuracy)
  print("confusion matrix:")
  print(cm)

  #saving the model to working directory as "model.pkl
  model_filename = "model.pkl"
  vectorizer_filename = 'vectorizer.pkl'
  with open(model_filename, 'wb') as file:
    pickle.dump(model, file)

  with open(vectorizer_filename, 'wb') as file:
    pickle.dump(vectorizer, file)